{
  "questions": [
    "What are the three hierarchical criteria derived from structuralist philosophy of science used in the paper to classify machine learning representations?",
    "What are the three hierarchical criteria derived from structuralist philosophy of science that the paper uses to classify the ontological commitments in machine learning research on neural network representations?",
    "How does the theory of category-equivariant neural networks (CENNs) extend the concept of equivariance beyond traditional group actions?",
    "What are the architectural limitations of current neural network paradigms in relation to achieving artificial general intelligence, according to the paper?",
    "What types of structures can be represented and approximated using category-equivariant neural networks (CENNs) as mentioned in the paper?",
    "How did the performance of deep learning models for differentiating true tumor progression from pseudoprogression vary between the first and second follow-up MRI scans in the study?",
    "What are the key architectural upgrades introduced in Qwen3-VL that enhance its multimodal reasoning capabilities?",
    "How does the proposed frequency-aware token reduction strategy mitigate the issues of rank collapsing and over-smoothing in Vision Transformers?",
    "What specific limitations did the researchers find in the performance of state-of-the-art language models when evaluated using the MindEval framework?",
    "What are the four evaluation tasks proposed in the ORIGAMISPACE benchmark for assessing multimodal large language models' spatial reasoning abilities?"
  ],
  "ground_truths": [
    "The three hierarchical criteria derived from structuralist philosophy of science used in the paper to classify machine learning representations are: entity elimination, source of structure, and mode of existence. These criteria help to analyze how machine learning models make implicit ontological commitments regarding their learned representations, revealing insights into the nature of these representations and the philosophical assumptions that underlie them.",
    "The three hierarchical criteria derived from structuralist philosophy of science used in the paper to classify the ontological commitments in machine learning research on neural network representations are: entity elimination, source of structure, and mode of existence. These criteria help in analyzing how different papers treat the nature of learned representations, particularly examining whether they view them as dependent on the model's architecture, data priors, and training dynamics, which aligns with the findings of a pronounced tendency toward structural idealism in the literature.",
    "The theory of category-equivariant neural networks (CENNs) extends the concept of equivariance by formulating it in a topological category with Radon measures, allowing for a broader range of symmetries to be captured. Unlike traditional equivariant networks that primarily focus on group actions, CENNs incorporate various structures such as posets, lattices, graphs, and cellular sheaves. This means that CENNs can handle not only geometric symmetries associated with groups but also contextual and compositional symmetries represented in different categorical settings. The paper demonstrates that finite-depth CENNs can approximate any continuous equivariant transformation, showcasing their ability to unify and generalize existing frameworks in equivariant deep learning.",
    "The paper argues that current neural network paradigms are fundamentally insufficient for achieving artificial general intelligence (AGI) due to their nature as static function approximators within a limited encoding framework. The authors contend that these networks, described metaphorically as 'sophisticated sponges,' exhibit complex behaviors but lack the structural richness and dynamic restructuring capabilities necessary for genuine understanding and intelligence. They critique the reliance on theoretical foundations such as the Universal Approximation Theorem, suggesting that this theorem addresses the wrong level of abstraction and does not account for the need for a more complex architectural organization. The paper calls for a framework that distinguishes between the computational substrate (existential facilities) and the interpretive structures (architectural organization) essential for developing a more robust form of machine intelligence.",
    "The paper indicates that category-equivariant neural networks (CENNs) can represent and approximate a variety of structures, including groups, groupoids, posets, lattices, graphs, and cellular sheaves. The authors propose a unifying framework that encompasses these different types of structures through the lens of equivariance, allowing CENNs to capture not just geometric symmetries but also other forms of contextual and compositional symmetries. The framework also provides universal approximation theorems for these categories, demonstrating that finite-depth CENNs can be densely approximated in the space of continuous equivariant transformations for these structures.",
    "The performance of deep learning models showed improvement in discrimination between true tumor progression (TP) and treatment-related pseudoprogression (PsP) when comparing the first and second follow-up MRI scans. The accuracies for both stages were comparable, ranging from approximately 0.70 to 0.74. However, at the second follow-up, the models exhibited increased F1 scores and area under the curve (AUC) values, indicating that the models were able to achieve richer separability of TP and PsP at this later time point in the care pathway. This suggests that the models' performance is stage-specific, with better results observed in later follow-up periods.",
    "Qwen3-VL introduces three key architectural upgrades that significantly enhance its multimodal reasoning capabilities: (i) an enhanced interleaved-MRoPE, which improves spatial-temporal modeling across images and video; (ii) DeepStack integration, which efficiently utilizes multi-level Vision Transformer (ViT) features to strengthen the alignment between vision and language; and (iii) text-based time alignment for video, which evolves from the previous T-RoPE mechanism to a more precise method of aligning textual timestamps, thereby improving temporal grounding in multimodal contexts. These upgrades collectively contribute to the model's ability to perform advanced reasoning tasks involving single and multi-image inputs, as well as video analysis.",
    "The proposed frequency-aware token reduction strategy addresses the issues of rank collapsing and over-smoothing by partitioning tokens into high-frequency and low-frequency categories. High-frequency tokens, which carry more critical information for the model's performance, are selectively preserved to maintain accuracy. In contrast, low-frequency tokens, which may contribute to redundancy and noise, are aggregated into a compact direct current token. This aggregation allows for the retention of essential low-frequency components while reducing the overall number of tokens processed, thereby improving computational efficiency. By focusing on the frequency characteristics of the tokens, the method effectively reduces the risk of rank collapsing and over-smoothing, which can occur when too many similar tokens are averaged or when the representation becomes overly simplistic. The experimental results demonstrate that this strategy enhances the model's performance while significantly lowering computational overhead.",
    "The researchers found that all evaluated state-of-the-art language models scored below 4 out of 6 on average, indicating significant limitations in their performance. The models exhibited particular weaknesses in problematic AI-specific patterns of communication, struggled with longer interactions, and were less effective when supporting patients with severe symptoms. This suggests that despite advancements in reasoning capabilities and model scale, these factors do not necessarily translate to improved performance in realistic multi-turn mental health therapy conversations.",
    "The ORIGAMISPACE benchmark proposes four evaluation tasks designed to assess the multi-step spatial reasoning abilities of multimodal large language models (MLLMs). These tasks are: 1) Pattern Prediction, which evaluates the model's ability to predict the crease pattern from given inputs; 2) Multi-step Spatial Reasoning, which tests the model's capability to reason through multiple steps in spatial configurations; 3) Spatial Relationship Prediction, which focuses on the model's understanding of spatial relationships between different components within the origami tasks; and 4) End-to-End CP Code Generation, which involves generating crease pattern codes based on the overall folding process. These tasks collectively aim to benchmark the models' performance in handling complex spatial reasoning and mathematical constraints inherent in origami-related challenges."
  ],
  "relevant_doc_ids": [
    [
      "2511.18633v1"
    ],
    [
      "2511.18633v1"
    ],
    [
      "2511.18417v1"
    ],
    [
      "2511.18517v1"
    ],
    [
      "2511.18417v1"
    ],
    [
      "2511.18595v1"
    ],
    [
      "2511.21631v1"
    ],
    [
      "2511.21477v1"
    ],
    [
      "2511.18491v2"
    ],
    [
      "2511.18450v1"
    ]
  ],
  "ground_truth_contexts": [
    [
      "Machine learning models increasingly function as representational systems, yet the philosoph- ical assumptions underlying their internal structures remain largely unexamined. This paper develops a structuralist decision framework for classifying the implicit ontological commitments made in machine learning research on neural network representations. Using a modified PRISMA protocol, a systematic review of the last two decades of literature on representation learning and interpretability is conducted. Five influential papers are analysed through three hierarchical criteria derived from structuralist philosophy of science: entity elimination, source of structure, and mode of existence. The results reveal a pronounced tendency toward structural idealism, where learned representations are treated as model-dependent constructions shaped by architec- ture, data priors, and training dynamics. Eliminative and non-eliminative structuralist stances appear selectively, while structural realism is notably absent. The proposed framework clarifies conceptual tensions in debates on interpretability, emergence, and epistemic trust in machine learning, and offers a rigorous foundation for future interdisciplinary work between philosophy of science and machine learning."
    ],
    [
      "Machine learning models increasingly function as representational systems, yet the philosoph- ical assumptions underlying their internal structures remain largely unexamined. This paper develops a structuralist decision framework for classifying the implicit ontological commitments made in machine learning research on neural network representations. Using a modified PRISMA protocol, a systematic review of the last two decades of literature on representation learning and interpretability is conducted. Five influential papers are analysed through three hierarchical criteria derived from structuralist philosophy of science: entity elimination, source of structure, and mode of existence. The results reveal a pronounced tendency toward structural idealism, where learned representations are treated as model-dependent constructions shaped by architec- ture, data priors, and training dynamics. Eliminative and non-eliminative structuralist stances appear selectively, while structural realism is notably absent. The proposed framework clarifies conceptual tensions in debates on interpretability, emergence, and epistemic trust in machine learning, and offers a rigorous foundation for future interdisciplinary work between philosophy of science and machine learning."
    ],
    [
      "We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries."
    ],
    [
      "Within the limited scope of this paper, we argue that artificial general intelligence cannot emerge from current neural network paradigms regardless of scale, nor is such an approach healthy for the field at present. Drawing on various notions, discussions, present-day developments and observations, current debates and critiques, experiments, and so on in between philosophy, including the Chinese Room Argument and G\u00f6delian argument, neuroscientific ideas, computer science, the theoretical consideration of artificial intelligence, and learning theory, we address conceptually that neural networks are architecturally insufficient for genuine understanding. They operate as static function approximators of a limited encoding framework - a 'sophisticated sponge' exhibiting complex behaviours without structural richness that constitute intelligence. We critique the theoretical foundations the field relies on and created of recent times; for example, an interesting heuristic as neural scaling law (as an example, arXiv:2001.08361 ) made prominent in a wrong way of interpretation, The Universal Approximation Theorem addresses the wrong level of abstraction and, in parts, partially, the question of current architectures lacking dynamic restructuring capabilities. We propose a framework distinguishing existential facilities (computational substrate) from architectural organization (interpretive structures), and outline principles for what genuine machine intelligence would require, and furthermore, a conceptual method of structuralizing the richer framework on which the principle of neural network system takes hold."
    ],
    [
      "We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries."
    ],
    [
      "Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts."
    ],
    [
      "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows."
    ],
    [
      "Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations."
    ],
    [
      "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data."
    ],
    [
      "Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models(MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances,each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks."
    ]
  ]
}