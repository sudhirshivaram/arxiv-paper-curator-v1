{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: arXiv API Integration & PDF Processing\n",
    "\n",
    "**What We're Building This Week:**\n",
    "\n",
    "Week 2 focuses on implementing the core data ingestion pipeline that will automatically fetch, process, and store arXiv papers. This is the foundation that feeds our RAG system with fresh academic content.\n",
    "\n",
    "## Week 2 Focus Areas\n",
    "\n",
    "### ðŸŽ¯ Core Objectives\n",
    "- **arXiv API Integration**: Build a robust client with rate limiting and retry logic\n",
    "- **PDF Processing Pipeline**: Download and parse scientific PDFs with structured content extraction\n",
    "- **Database Storage**: Persist paper metadata and content in PostgreSQL\n",
    "- **Error Handling**: Implement comprehensive error handling and graceful degradation\n",
    "- **Automation Ready**: Prepare components for Airflow orchestration\n",
    "\n",
    "### ðŸ”§ What We'll Test In This Notebook\n",
    "1. **arXiv API Client** - Fetch CS.AI papers with proper rate limiting\n",
    "2. **PDF Download System** - Download and cache PDFs with error handling  \n",
    "3. **Docling PDF Parser** - Extract structured content (sections, tables, figures)\n",
    "4. **Database Integration** - Store and retrieve papers from PostgreSQL\n",
    "5. **Complete Pipeline** - End-to-end processing from arXiv to database\n",
    "6. **Production Readiness** - Error handling, logging, and performance metrics\n",
    "\n",
    "\n",
    "### ðŸ“Š Success Metrics\n",
    "- arXiv API calls succeed with proper rate limiting\n",
    "- PDF download and caching works reliably  \n",
    "- Docling extracts structured content from scientific PDFs\n",
    "- Database stores complete paper metadata\n",
    "- Pipeline handles errors gracefully and continues processing\n",
    "- All components ready for Airflow automation (Week 2+)\n",
    "\n",
    "---\n",
    "\n",
    "## Week 2 Component Status\n",
    "| Component | Purpose | Status |\n",
    "|-----------|---------|--------|\n",
    "| **arXiv API Client** | Fetch CS.AI papers with rate limiting | âœ… Complete |\n",
    "| **PDF Downloader** | Download and cache PDFs locally | âœ… Complete |\n",
    "| **Docling Parser** | Extract structured content from PDFs | âœ… Complete |\n",
    "| **Metadata Fetcher** | Orchestrate complete pipeline | âœ… Complete |\n",
    "| **Database Storage** | Store papers in PostgreSQL | âš ï¸ Needs volume refresh |\n",
    "| **Airflow DAGs** | Automate daily ingestion | âš ï¸ Needs container update |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ IMPORTANT: Week 2 Database Schema Update\n",
    "\n",
    "**NEW USERS OR SCHEMA CONFLICTS**: If you're starting Week 2 fresh or experiencing database schema conflicts, use this clean start approach:\n",
    "\n",
    "### Fresh Start (Recommended for Week 2)\n",
    "```bash\n",
    "# Complete clean slate - removes all data but ensures correct schema\n",
    "docker compose down -v\n",
    "\n",
    "# Build fresh containers with latest code\n",
    "docker compose up --build -d\n",
    "```\n",
    "\n",
    "**When to use this:**\n",
    "- First time running Week 2 \n",
    "- Schema errors or column missing errors\n",
    "- Want to start with clean database\n",
    "- Previous Week 1 data not important\n",
    "\n",
    "**Note**: This destroys existing data but ensures you have the correct Week 2 schema with all new columns for PDF processing and arXiv metadata.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Check\n",
    "\n",
    "**Before starting:**\n",
    "1. Week 1 infrastructure completed\n",
    "2. UV environment activated\n",
    "3. Docker Desktop running\n",
    "\n",
    "**Why fresh containers?** Week 2 includes new Airflow dependencies and code changes that require rebuilding images rather than using cached layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 2 CONTAINER & SERVICE HEALTH CHECK\n",
      "==================================================\n",
      "Project root: /home/bhargav/arxiv-paper-curator\n",
      "\n",
      "1. Checking container status...\n",
      "âœ“ Containers are running:\n",
      "   NAME                    IMAGE                                            COMMAND                  SERVICE                 CREATED         STATUS                     PORTS\n",
      "   rag-airflow             arxiv-paper-curator-airflow                      \"/entrypoint.sh\"         airflow                 8 minutes ago   Up 8 minutes (healthy)     0.0.0.0:8080->8080/tcp\n",
      "   rag-api                 arxiv-paper-curator-api                          \"uvicorn src.main:apâ€¦\"   api                     8 minutes ago   Up 6 minutes (healthy)     0.0.0.0:8000->8000/tcp\n",
      "   rag-clickhouse          clickhouse/clickhouse-server:24.8-alpine         \"/entrypoint.sh\"         clickhouse              8 minutes ago   Up 8 minutes (healthy)     8123/tcp, 9000/tcp, 9009/tcp\n",
      "   rag-dashboards          opensearchproject/opensearch-dashboards:2.19.0   \"./opensearch-dashboâ€¦\"   opensearch-dashboards   8 minutes ago   Up 8 minutes (healthy)     0.0.0.0:5601->5601/tcp\n",
      "   rag-langfuse            langfuse/langfuse:2                              \"dumb-init -- ./web/â€¦\"   langfuse                8 minutes ago   Up 8 minutes (unhealthy)   0.0.0.0:3000->3000/tcp\n",
      "   rag-langfuse-postgres   postgres:16-alpine                               \"docker-entrypoint.sâ€¦\"   langfuse-postgres       8 minutes ago   Up 8 minutes (healthy)     5432/tcp\n",
      "   rag-ollama              ollama/ollama:0.11.2                             \"/bin/ollama serve\"      ollama                  8 minutes ago   Up 8 minutes (healthy)     0.0.0.0:11434->11434/tcp\n",
      "   rag-opensearch          opensearchproject/opensearch:2.19.0              \"./opensearch-dockerâ€¦\"   opensearch              8 minutes ago   Up 8 minutes (healthy)     0.0.0.0:9200->9200/tcp, 9300/tcp, 0.0.0.0:9600->9600/tcp, 9650/tcp\n",
      "   rag-pgadmin             dpage/pgadmin4:latest                            \"/entrypoint.sh\"         pgadmin                 8 minutes ago   Up 8 minutes               443/tcp, 0.0.0.0:5050->80/tcp\n",
      "   rag-postgres            postgres:16-alpine                               \"docker-entrypoint.sâ€¦\"   postgres                8 minutes ago   Up 8 minutes (healthy)     0.0.0.0:5432->5432/tcp\n",
      "   rag-redis               redis:7-alpine                                   \"docker-entrypoint.sâ€¦\"   redis                   8 minutes ago   Up 8 minutes (healthy)     0.0.0.0:6379->6379/tcp\n",
      "\n",
      "2. Checking service health...\n",
      "âœ“ FastAPI: Healthy\n",
      "âœ“ PostgreSQL (via API): Healthy\n",
      "âœ“ Ollama: Healthy\n",
      "âœ“ OpenSearch: Healthy\n",
      "âœ“ Airflow: Healthy\n",
      "\n",
      "==================================================\n",
      "âœ“ ALL SERVICES HEALTHY! Ready for Week 2 development.\n"
     ]
    }
   ],
   "source": [
    "# Check if Fresh Containers are Built and All Services Healthy\n",
    "import subprocess\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"WEEK 2 CONTAINER & SERVICE HEALTH CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week2\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    print(\"âœ— Could not find project root\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Step 1: Check if containers are built and running\n",
    "print(\"\\n1. Checking container status...\")\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"table\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        print(\"âœ“ Containers are running:\")\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            print(f\"   {line}\")\n",
    "    else:\n",
    "        print(\"âœ— No containers running or docker compose failed\")\n",
    "        print(\"Please run the build commands from the markdown cell above\")\n",
    "        exit()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error checking containers: {e}\")\n",
    "    print(\"Please run the build commands from the markdown cell above\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Check all service health (corrected endpoints)\n",
    "print(\"\\n2. Checking service health...\")\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\", \n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\"\n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ“ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"âœ— {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"âœ— {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if all_healthy:\n",
    "    print(\"âœ“ ALL SERVICES HEALTHY! Ready for Week 2 development.\")\n",
    "else:\n",
    "    print(\"âœ— Some services need attention.\")\n",
    "    print(\"If you just rebuilt containers, wait 1-2 minutes and run this cell again.\")\n",
    "    print(\"Airflow and OpenSearch take longest to start up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WEEK 2 CONTAINER & SERVICE HEALTH CHECK\n",
      "======================================================================\n",
      "Project root: /home/bhargav/arxiv-paper-curator/notebooks\n",
      "\n",
      "1. Checking container status...\n",
      "âœ“ 11 containers running\n",
      "\n",
      "2. Checking service health...\n",
      "âœ“ FastAPI: Healthy\n",
      "âœ“ Ollama: Healthy\n",
      "âœ“ OpenSearch: Healthy\n",
      "âœ“ Airflow: Healthy\n",
      "âœ“ Langfuse: Healthy\n",
      "\n",
      "3. Checking PostgreSQL...\n",
      "âœ“ PostgreSQL: Connected (48 tables)\n",
      "\n",
      "4. Checking Redis...\n",
      "âœ“ Redis: Connected (0 keys)\n",
      "\n",
      "5. API Service Details...\n",
      "  Version: 0.1.0\n",
      "  Environment: development\n",
      "  Dependent Services:\n",
      "    âœ— database\n",
      "    âœ— opensearch\n",
      "    âœ— ollama\n",
      "\n",
      "======================================================================\n",
      "âœ“ All services are healthy and ready!\n",
      "\n",
      "Access URLs:\n",
      "  â€¢ API Docs:     http://localhost:8000/docs\n",
      "  â€¢ Airflow:      http://localhost:8080 (admin/admin)\n",
      "  â€¢ OpenSearch:   http://localhost:5601\n",
      "  â€¢ Langfuse:     http://localhost:3000\n",
      "  â€¢ pgAdmin:      http://localhost:5050\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nWEEK 2 CONTAINER & SERVICE HEALTH CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"week2\" else Path.cwd()\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# 1. Check containers\n",
    "print(\"\\n1. Checking container status...\")\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"compose\", \"ps\"],\n",
    "    cwd=str(project_root),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    # Count running containers\n",
    "    running_count = result.stdout.count(\"Up\")\n",
    "    print(f\"âœ“ {running_count} containers running\")\n",
    "else:\n",
    "    print(\"âœ— Could not get container status\")\n",
    "    exit(1)\n",
    "\n",
    "# 2. Check service health with retries\n",
    "print(\"\\n2. Checking service health...\")\n",
    "\n",
    "services = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"Ollama\": \"http://localhost:11434/api/tags\",\n",
    "    \"OpenSearch\": \"http://localhost:9200\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\",\n",
    "    \"Langfuse\": \"http://localhost:3000/api/public/health\",\n",
    "}\n",
    "\n",
    "def check_service(name, url, retries=3, timeout=5):\n",
    "    \"\"\"Check service with retries\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                return True, \"Healthy\"\n",
    "            else:\n",
    "                return False, f\"Status {response.status_code}\"\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                return False, \"Not accessible\"\n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                return False, \"Timeout\"\n",
    "        except Exception as e:\n",
    "            return False, str(e)\n",
    "    return False, \"Unknown error\"\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for name, url in services.items():\n",
    "    healthy, status = check_service(name, url)\n",
    "    if healthy:\n",
    "        print(f\"âœ“ {name}: {status}\")\n",
    "    else:\n",
    "        print(f\"âœ— {name}: {status}\")\n",
    "        all_healthy = False\n",
    "\n",
    "# 3. Check PostgreSQL\n",
    "print(\"\\n3. Checking PostgreSQL...\")\n",
    "try:\n",
    "    import psycopg2\n",
    "    conn = psycopg2.connect(\n",
    "        host='localhost', port=5432,\n",
    "        database='rag_db', user='rag_user', password='rag_password',\n",
    "        connect_timeout=5\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'\")\n",
    "    table_count = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(f\"âœ“ PostgreSQL: Connected ({table_count} tables)\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— PostgreSQL: {e}\")\n",
    "    all_healthy = False\n",
    "\n",
    "# 4. Check Redis\n",
    "print(\"\\n4. Checking Redis...\")\n",
    "try:\n",
    "    import redis\n",
    "    r = redis.Redis(host='localhost', port=6379, socket_connect_timeout=5)\n",
    "    r.ping()\n",
    "    key_count = r.dbsize()\n",
    "    print(f\"âœ“ Redis: Connected ({key_count} keys)\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Redis: {e}\")\n",
    "    all_healthy = False\n",
    "\n",
    "# 5. Check API details\n",
    "print(\"\\n5. API Service Details...\")\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/api/v1/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"  Version: {data.get('version', 'N/A')}\")\n",
    "        print(f\"  Environment: {data.get('environment', 'N/A')}\")\n",
    "        \n",
    "        if 'services' in data:\n",
    "            print(f\"  Dependent Services:\")\n",
    "            for svc, status in data.get('services', {}).items():\n",
    "                svc_ok = status.get('status') == 'ok' if isinstance(status, dict) else bool(status)\n",
    "                indicator = \"âœ“\" if svc_ok else \"âœ—\"\n",
    "                print(f\"    {indicator} {svc}\")\n",
    "except Exception as e:\n",
    "    print(f\"  âš  Could not get API details: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_healthy:\n",
    "    print(\"âœ“ All services are healthy and ready!\")\n",
    "    print(\"\\nAccess URLs:\")\n",
    "    print(\"  â€¢ API Docs:     http://localhost:8000/docs\")\n",
    "    print(\"  â€¢ Airflow:      http://localhost:8080 (admin/admin)\")\n",
    "    print(\"  â€¢ OpenSearch:   http://localhost:5601\")\n",
    "    print(\"  â€¢ Langfuse:     http://localhost:3000\")\n",
    "    print(\"  â€¢ pgAdmin:      http://localhost:5050\")\n",
    "else:\n",
    "    print(\"âš  Some services need attention.\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check logs: docker compose logs <service-name>\")\n",
    "    print(\"  2. Restart: docker compose restart <service-name>\")\n",
    "    print(\"  3. Full restart: docker compose down && docker compose up -d\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.3\n",
      "Environment: /home/bhargav/arxiv-paper-curator/.venv/bin/python3\n",
      "âœ“ Project root: /home/bhargav/arxiv-paper-curator\n"
     ]
    }
   ],
   "source": [
    "# Environment Check\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Environment: {sys.executable}\")\n",
    "\n",
    "# Find project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week2\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = None\n",
    "\n",
    "if project_root and (project_root / \"compose.yml\").exists():\n",
    "    print(f\"âœ“ Project root: {project_root}\")\n",
    "    # Add project to Python path\n",
    "    sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"âœ— Missing compose.yml - check directory\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Health Verification\n",
    "\n",
    "Ensure all services from Week 1 are still running correctly:\n",
    "\n",
    "### ðŸ”— Service Access Points\n",
    "- **FastAPI**: http://localhost:8000/docs (API documentation)\n",
    "- **PostgreSQL**: via API or `docker exec -it rag-postgres psql -U rag_user -d rag_db`\n",
    "- **OpenSearch**: http://localhost:9200/_cluster/health\n",
    "- **Ollama**: http://localhost:11434 (LLM service)\n",
    "- **Airflow**: http://localhost:8080 (Username: `admin`, Password: `admin`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 2 PREREQUISITE CHECK\n",
      "==================================================\n",
      "âœ“ FastAPI: Healthy\n",
      "âœ“ PostgreSQL (via API): Healthy\n",
      "âœ“ Ollama: Healthy\n",
      "âœ“ OpenSearch: Healthy\n",
      "âœ“ Airflow: Healthy\n",
      "\n",
      "All services healthy! Ready for Week 2 development.\n"
     ]
    }
   ],
   "source": [
    "# Test Service Connectivity\n",
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\", \n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\"  \n",
    "}\n",
    "\n",
    "print(\"WEEK 2 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ“ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"âœ— {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"âœ— {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "print()\n",
    "if all_healthy:\n",
    "    print(\"All services healthy! Ready for Week 2 development.\")\n",
    "else:\n",
    "    print(\"Some services need attention. Check Week 1 notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. arXiv API Client Testing\n",
    "\n",
    "Test the arXiv API client with rate limiting and retry logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING ARXIV API CLIENT\n",
      "========================================\n",
      "âœ“ Client created: https://export.arxiv.org/api/query\n",
      "   Rate limit: 3.0s\n",
      "   Max results: 15\n",
      "   Category: cs.AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test arXiv API Client\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import our arXiv client\n",
    "from src.services.arxiv.factory import make_arxiv_client\n",
    "\n",
    "print(\"TESTING ARXIV API CLIENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create client\n",
    "arxiv_client = make_arxiv_client()\n",
    "print(f\"âœ“ Client created: {arxiv_client.base_url}\")\n",
    "print(f\"   Rate limit: {arxiv_client.rate_limit_delay}s\")\n",
    "print(f\"   Max results: {arxiv_client.max_results}\")\n",
    "print(f\"   Category: {arxiv_client.search_category}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Fetch Recent CS.AI Papers\n",
      "âœ“ Fetched 2 papers\n",
      "   1. [2511.19436v1] VDC-Agent: When Video Detailed Captioners Evolve Themselves ...\n",
      "      Authors: Qiang Wang, Xinyuan Gao...\n",
      "      Categories: cs.CV, cs.AI, cs.LG, cs.MM\n",
      "      Published: 2025-11-24T18:59:56Z\n",
      "\n",
      "   2. [2511.19433v1] Mixture of Horizons in Action Chunking...\n",
      "      Authors: Dong Jing, Gang Wang...\n",
      "      Categories: cs.RO, cs.AI, cs.CV\n",
      "      Published: 2025-11-24T18:59:51Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Paper Fetching\n",
    "async def test_paper_fetching():\n",
    "    \"\"\"Test fetching papers from arXiv with rate limiting.\"\"\"\n",
    "    \n",
    "    print(\"Test 1: Fetch Recent CS.AI Papers\")\n",
    "    try:\n",
    "        papers = await arxiv_client.fetch_papers(\n",
    "            max_results=2, \n",
    "            sort_by=\"submittedDate\",\n",
    "            sort_order=\"descending\"\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Fetched {len(papers)} papers\")\n",
    "        \n",
    "        if papers:\n",
    "            for i, paper in enumerate(papers[:2], 1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\")\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "        \n",
    "        return papers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error fetching papers: {e}\")\n",
    "        if \"503\" in str(e):\n",
    "            print(\"   arXiv API temporarily unavailable (normal)\")\n",
    "            print(\"   Rate limiting and error handling working correctly\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "papers = await test_paper_fetching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Date Range Filtering\n",
      "âœ“ Date filtering test: 5 papers from 20250808-20250809\n",
      "   1. [2508.07111v1] Investigating Intersectional Bias in Large Language Models u...\n",
      "      Authors: Falaah Arif Khan, Nivedha Sivakumar...\n",
      "      Categories: cs.CL, cs.AI\n",
      "      Published: 2025-08-09T22:24:40Z\n",
      "\n",
      "   2. [2508.07107v2] Designing a Feedback-Driven Decision Support System for Dyna...\n",
      "      Authors: Timothy Oluwapelumi Adeyemi, Nadiah Fahad AlOtaibi\n",
      "      Categories: cs.AI, cs.CY\n",
      "      Published: 2025-08-09T21:24:54Z\n",
      "\n",
      "   3. [2508.07102v1] Towards High-Order Mean Flow Generative Models: Feasibility,...\n",
      "      Authors: Yang Cao, Yubin Chen...\n",
      "      Categories: cs.LG, cs.AI, cs.CV\n",
      "      Published: 2025-08-09T21:10:58Z\n",
      "\n",
      "   4. [2508.07101v1] Less Is More: Training-Free Sparse Attention with Global Loc...\n",
      "      Authors: Lijie Yang, Zhihao Zhang...\n",
      "      Categories: cs.CL, cs.AI\n",
      "      Published: 2025-08-09T21:10:33Z\n",
      "\n",
      "   5. [2508.07095v1] Hide or Highlight: Understanding the Impact of Factuality Ex...\n",
      "      Authors: Hyo Jin Do, Werner Geyer\n",
      "      Categories: cs.HC, cs.AI\n",
      "      Published: 2025-08-09T20:45:21Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Date Filtering\n",
    "async def test_date_filtering():\n",
    "    \"\"\"Test date range filtering functionality.\"\"\"\n",
    "    \n",
    "    print(\"Test 2: Date Range Filtering\")\n",
    "    \n",
    "    # Use specific dates: \n",
    "    from_date = \"20250808\"  \n",
    "    to_date = \"20250809\"    \n",
    "    try:\n",
    "        date_papers = await arxiv_client.fetch_papers(\n",
    "            max_results=5,\n",
    "            from_date=from_date,\n",
    "            to_date=to_date\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Date filtering test: {len(date_papers)} papers from {from_date}-{to_date}\")\n",
    "        \n",
    "        if date_papers:\n",
    "            for i, paper in enumerate(date_papers, 1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\")\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "        \n",
    "        return date_papers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Date filtering error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run date filtering test\n",
    "date_papers = await test_date_filtering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Download and Caching\n",
    "\n",
    "Test PDF download functionality with caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: PDF Download & Caching\n",
      "Testing PDF download for: 2508.07111v1\n",
      "Title: Investigating Intersectional Bias in Large Language Models u...\n",
      "âœ“ PDF downloaded: 2508.07111v1.pdf (6.81 MB)\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Download\n",
    "async def test_pdf_download(test_papers):\n",
    "    \"\"\"Test PDF downloading with caching.\"\"\"\n",
    "\n",
    "    print(\"Test 3: PDF Download & Caching\")\n",
    "    \n",
    "    if not test_papers:\n",
    "        print(\"No papers available for PDF download test\")\n",
    "        return None\n",
    "    \n",
    "    # Test with first paper\n",
    "    test_paper = test_papers[0]\n",
    "    print(f\"Testing PDF download for: {test_paper.arxiv_id}\")\n",
    "    print(f\"Title: {test_paper.title[:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Download PDF \n",
    "        pdf_path = await arxiv_client.download_pdf(test_paper)\n",
    "        \n",
    "        if pdf_path and pdf_path.exists():\n",
    "            size_mb = pdf_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"âœ“ PDF downloaded: {pdf_path.name} ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(\"âœ— PDF download failed\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— PDF download error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run PDF download test \n",
    "pdf_path = await test_pdf_download(date_papers[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Docling PDF Processing\n",
    "\n",
    "Test PDF parsing with Docling for structured content extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:49:45,919 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4: PDF Parsing with Docling\n",
      "========================================\n",
      "PDF parser service created\n",
      "Config: 30 pages, 20MB\n",
      "\n",
      "Found 4 PDF files to test parsing\n",
      "Testing PDF parsing with: 2508.11112v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:49:46,024 - INFO - Going to convert document batch...\n",
      "2025-11-25 18:49:46,025 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 60c8066c482b9239b869b997da3fb1da\n",
      "2025-11-25 18:49:46,073 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-25 18:49:46,078 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-11-25 18:49:46,142 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-25 18:49:46,200 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-11-25 18:49:46,261 - INFO - Accelerator device: 'cpu'\n",
      "2025-11-25 18:49:48,915 - INFO - Accelerator device: 'cpu'\n",
      "2025-11-25 18:49:50,604 - INFO - Processing document 2508.11112v1.pdf\n",
      "2025-11-25 18:52:39,738 - INFO - Finished converting document 2508.11112v1.pdf in 173.82 sec.\n",
      "2025-11-25 18:52:39,957 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-11-25 18:52:39,983 - INFO - Parsed 2508.11112v1.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF parsing successful!\n",
      "  Sections: 29\n",
      "  Raw text length: 68452 characters\n",
      "  Parser used: ParserType.DOCLING\n",
      "  First section: 'Content' (40 chars)\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Parsing with Docling\n",
    "from src.services.pdf_parser.factory import make_pdf_parser_service\n",
    "from src.config import get_settings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Test 4: PDF Parsing with Docling\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create PDF parser\n",
    "pdf_parser = make_pdf_parser_service()\n",
    "settings = get_settings()\n",
    "print(\"PDF parser service created\")\n",
    "print(f\"Config: {settings.pdf_parser.max_pages} pages, {settings.pdf_parser.max_file_size_mb}MB\")\n",
    "\n",
    "# Test parsing with actual PDF files\n",
    "cache_dir = Path(\"data/arxiv_pdfs\")\n",
    "if cache_dir.exists():\n",
    "    pdf_files = list(cache_dir.glob(\"*.pdf\"))\n",
    "    print(f\"\\nFound {len(pdf_files)} PDF files to test parsing\")\n",
    "    \n",
    "    if pdf_files:\n",
    "        # Test parsing the first PDF\n",
    "        test_pdf = pdf_files[0]\n",
    "        print(f\"Testing PDF parsing with: {test_pdf.name}\")\n",
    "        \n",
    "        try:\n",
    "            pdf_content = await pdf_parser.parse_pdf(test_pdf)\n",
    "            \n",
    "            if pdf_content:\n",
    "                print(f\"âœ“ PDF parsing successful!\")\n",
    "                print(f\"  Sections: {len(pdf_content.sections)}\")\n",
    "                print(f\"  Raw text length: {len(pdf_content.raw_text)} characters\")\n",
    "                print(f\"  Parser used: {pdf_content.parser_used}\")\n",
    "                \n",
    "                # Show first section as example\n",
    "                if pdf_content.sections:\n",
    "                    first_section = pdf_content.sections[0]\n",
    "                    print(f\"  First section: '{first_section.title}' ({len(first_section.content)} chars)\")\n",
    "            else:\n",
    "                print(\"âœ— PDF parsing failed (Docling compatibility issue)\")\n",
    "                print(\"This is expected - not all PDFs work with Docling\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— PDF parsing error: {e}\")\n",
    "            print(\"This demonstrates the error handling in action\")\n",
    "    else:\n",
    "        print(\"No PDF files available for parsing test\")\n",
    "else:\n",
    "    print(\"No PDF cache directory found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Storage Testing\n",
    "\n",
    "Test storing papers in PostgreSQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:52:41,935 - INFO - Attempting to connect to PostgreSQL at: postgres:5432/rag_db\n",
      "2025-11-25 18:52:41,976 - ERROR - Failed to initialize PostgreSQL database: (psycopg2.OperationalError) could not translate host name \"postgres\" to address: Temporary failure in name resolution\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5: Database Storage\n",
      "========================================\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) could not translate host name \"postgres\" to address: Temporary failure in name resolution\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:143\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:3301\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3281\u001b[39m \n\u001b[32m   3282\u001b[39m \u001b[33;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3299\u001b[39m \n\u001b[32m   3300\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:447\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m \n\u001b[32m    446\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:711\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/impl.py:175\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:388\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:673\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:899\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:895\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    894\u001b[39m \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/create.py:661\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    659\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:629\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOperationalError\u001b[39m: could not translate host name \"postgres\" to address: Temporary failure in name resolution\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create database connection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m database = \u001b[43mmake_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Database connection created\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m papers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/src/db/factory.py:25\u001b[39m, in \u001b[36mmake_database\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m config = PostgreSQLSettings(\n\u001b[32m     18\u001b[39m     database_url=settings.postgres_database_url,\n\u001b[32m     19\u001b[39m     echo_sql=settings.postgres_echo_sql,\n\u001b[32m     20\u001b[39m     pool_size=settings.postgres_pool_size,\n\u001b[32m     21\u001b[39m     max_overflow=settings.postgres_max_overflow,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m database = PostgreSQLDatabase(config=config)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mdatabase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m database\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/src/db/interfaces/postgresql.py:46\u001b[39m, in \u001b[36mPostgreSQLDatabase.startup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Test the connection\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m     47\u001b[39m     conn.execute(text(\u001b[33m\"\u001b[39m\u001b[33mSELECT 1\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     48\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mDatabase connection test successful\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:3277\u001b[39m, in \u001b[36mEngine.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Connection:\n\u001b[32m   3255\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[32m   3256\u001b[39m \n\u001b[32m   3257\u001b[39m \u001b[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3274\u001b[39m \n\u001b[32m   3275\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    143\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = engine.raw_connection()\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[43mConnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2440\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception_noconnection\u001b[39m\u001b[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[39m\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2439\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2440\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2441\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2442\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:143\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    145\u001b[39m         Connection._handle_dbapi_exception_noconnection(\n\u001b[32m    146\u001b[39m             err, dialect, engine\n\u001b[32m    147\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:3301\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m   3280\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3281\u001b[39m \n\u001b[32m   3282\u001b[39m \u001b[33;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3299\u001b[39m \n\u001b[32m   3300\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:447\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m    440\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m \n\u001b[32m    446\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1256\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_checkout\u001b[39m(\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1261\u001b[39m     fairy: Optional[_ConnectionFairy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1262\u001b[39m ) -> _ConnectionFairy:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m         fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:711\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    709\u001b[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    714\u001b[39m     dbapi_connection = rec.get_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection()\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/impl.py:175\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inc_overflow():\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:388\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ConnectionPoolEntry:\n\u001b[32m    386\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:673\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28mself\u001b[39m.__pool = pool\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:899\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    902\u001b[39m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[32m    903\u001b[39m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py:895\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m     pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n\u001b[32m    897\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/create.py:661\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    658\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    659\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:629\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arxiv-paper-curator/.venv/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: (psycopg2.OperationalError) could not translate host name \"postgres\" to address: Temporary failure in name resolution\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "# Test Database Storage\n",
    "from src.db.factory import make_database\n",
    "from src.repositories.paper import PaperRepository\n",
    "from src.schemas.arxiv.paper import PaperCreate\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "print(\"Test 5: Database Storage\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create database connection\n",
    "database = make_database()\n",
    "print(\"âœ“ Database connection created\")\n",
    "\n",
    "if papers:\n",
    "    test_paper = papers[0]\n",
    "    print(f\"Storing paper: {test_paper.arxiv_id}\")\n",
    "    \n",
    "    try:\n",
    "        with database.get_session() as session:\n",
    "            paper_repo = PaperRepository(session)\n",
    "            \n",
    "            # Convert to database format\n",
    "            published_date = date_parser.parse(test_paper.published_date) if isinstance(test_paper.published_date, str) else test_paper.published_date\n",
    "            \n",
    "            paper_create = PaperCreate(\n",
    "                arxiv_id=test_paper.arxiv_id,\n",
    "                title=test_paper.title,\n",
    "                authors=test_paper.authors,\n",
    "                abstract=test_paper.abstract,\n",
    "                categories=test_paper.categories,\n",
    "                published_date=published_date,\n",
    "                pdf_url=test_paper.pdf_url\n",
    "            )\n",
    "            \n",
    "            # Store paper (upsert to avoid duplicates)\n",
    "            stored_paper = paper_repo.upsert(paper_create)\n",
    "            \n",
    "            if stored_paper:\n",
    "                print(f\"âœ“ Paper stored with ID: {stored_paper.id}\")\n",
    "                print(f\"   Database ID: {stored_paper.id}\")\n",
    "                print(f\"   arXiv ID: {stored_paper.arxiv_id}\")\n",
    "                print(f\"   Title: {stored_paper.title[:50]}...\")\n",
    "                print(f\"   Authors: {len(stored_paper.authors)} authors\")\n",
    "                print(f\"   Categories: {', '.join(stored_paper.categories)}\")\n",
    "                \n",
    "                # Test retrieval\n",
    "                retrieved_paper = paper_repo.get_by_arxiv_id(test_paper.arxiv_id)\n",
    "                if retrieved_paper:\n",
    "                    print(f\"âœ“ Paper retrieval test passed\")\n",
    "                else:\n",
    "                    print(f\"âœ— Paper retrieval failed\")\n",
    "            else:\n",
    "                print(\"âœ— Paper storage failed\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Database error: {e}\")\n",
    "else:\n",
    "    print(\"No papers available for database storage test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Complete Pipeline\n",
    "from src.services.metadata_fetcher import make_metadata_fetcher\n",
    "\n",
    "print(\"Test 6: Complete Metadata Fetcher Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create metadata fetcher\n",
    "metadata_fetcher = make_metadata_fetcher(arxiv_client, pdf_parser)\n",
    "print(\"âœ“ Metadata fetcher service created\")\n",
    "\n",
    "# Test with small batch\n",
    "print(\"Running small batch test (2 papers, no PDF processing for speed)...\")\n",
    "\n",
    "try:\n",
    "    with database.get_session() as session:\n",
    "        results = await metadata_fetcher.fetch_and_process_papers(\n",
    "            max_results=2,  \n",
    "            process_pdfs=False,  \n",
    "            store_to_db=True,\n",
    "            db_session=session\n",
    "        )\n",
    "    \n",
    "    print(\"\\nPIPELINE RESULTS:\")\n",
    "    print(f\"   Papers fetched: {results.get('papers_fetched', 0)}\")\n",
    "    print(f\"   PDFs downloaded: {results.get('pdfs_downloaded', 0)}\")\n",
    "    print(f\"   PDFs parsed: {results.get('pdfs_parsed', 0)}\")\n",
    "    print(f\"   Papers stored: {results.get('papers_stored', 0)}\")\n",
    "    print(f\"   Processing time: {results.get('processing_time', 0):.1f}s\")\n",
    "    print(f\"   Errors: {len(results.get('errors', []))}\")\n",
    "    \n",
    "    if results.get('errors'):\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for error in results.get('errors', [])[:3]:  # Show first 3 errors\n",
    "            print(f\"   - {error}\")\n",
    "    \n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        print(\"\\nâœ“ Pipeline test successful!\")\n",
    "    else:\n",
    "        print(\"\\nNo papers fetched - may be arXiv API unavailability\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Airflow DAGs\n",
    "print(\"Test 7: Airflow DAG Status\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"  Airflow UI Access:\")\n",
    "print(\"   URL: http://localhost:8080\")\n",
    "print(\"   Username: admin\")\n",
    "print(\"   Password: admin\")\n",
    "print()\n",
    "\n",
    "# Check DAG status using docker exec\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"exec\", \"rag-airflow\", \"airflow\", \"dags\", \"list\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        dag_lines = [line for line in lines if 'arxiv' in line.lower() or 'hello' in line.lower()]\n",
    "        \n",
    "        print(\"Available DAGs:\")\n",
    "        for line in dag_lines:\n",
    "            if '|' in line:\n",
    "                parts = [part.strip() for part in line.split('|')]\n",
    "                if len(parts) >= 3:\n",
    "                    dag_id = parts[0]\n",
    "                    is_paused = parts[2]\n",
    "                    status = \"Active\" if is_paused == \"False\" else \"Paused\"\n",
    "                    print(f\"   - {dag_id}: {status}\")\n",
    "        \n",
    "        # Check for import errors\n",
    "        error_result = subprocess.run(\n",
    "            [\"docker\", \"exec\", \"rag-airflow\", \"airflow\", \"dags\", \"list-import-errors\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if \"docling\" in error_result.stderr:\n",
    "            print(\"\\nKnown Issue: Docling not installed in Airflow container\")\n",
    "            print(\"   - This is expected for Week 2\")\n",
    "            print(\"   - DAG structure is complete, runtime needs container fix\")\n",
    "            print(\"   - Solution: Add docling to Airflow container startup\")\n",
    "        elif error_result.returncode == 0:\n",
    "            print(\"\\nâœ“ No DAG import errors found\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âœ— Could not list DAGs: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Airflow test error: {e}\")\n",
    "\n",
    "print(\"\\n  To view DAGs graphically:\")\n",
    "print(\"   1. Open http://localhost:8080 in your browser\")\n",
    "print(\"   2. Login with admin/admin\")\n",
    "print(\"   3. Click on 'arxiv_paper_ingestion' DAG to see the workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Complete Pipeline with PDF Processing\n",
    "print(\"Test 8: Complete Pipeline with PDF Processing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Reuse metadata fetcher from Test 6\n",
    "print(\"âœ“ Using metadata fetcher service from previous test\")\n",
    "\n",
    "# Test with small batch including PDF processing\n",
    "print(\"Running enhanced test (3 papers with PDF processing)...\")\n",
    "\n",
    "try:\n",
    "    with database.get_session() as session:\n",
    "        results = await metadata_fetcher.fetch_and_process_papers(\n",
    "            max_results=3,  # Small batch\n",
    "            from_date=\"20250813\",  # Recent date\n",
    "            to_date=\"20250814\",\n",
    "            process_pdfs=True,  \n",
    "            store_to_db=True,\n",
    "            db_session=session\n",
    "        )\n",
    "    \n",
    "    print(\"\\nENHANCED PIPELINE RESULTS:\")\n",
    "    print(f\"   Papers fetched: {results.get('papers_fetched', 0)}\")\n",
    "    print(f\"   PDFs downloaded: {results.get('pdfs_downloaded', 0)}\")\n",
    "    print(f\"   PDFs parsed: {results.get('pdfs_parsed', 0)}\")\n",
    "    print(f\"   Papers stored: {results.get('papers_stored', 0)}\")\n",
    "    print(f\"   Processing time: {results.get('processing_time', 0):.1f}s\")\n",
    "    print(f\"   Errors: {len(results.get('errors', []))}\")\n",
    "    \n",
    "    # Show success rates\n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        download_rate = (results['pdfs_downloaded'] / results['papers_fetched']) * 100\n",
    "        parse_rate = (results['pdfs_parsed'] / results['pdfs_downloaded']) * 100 if results.get('pdfs_downloaded', 0) > 0 else 0\n",
    "        print(f\"   Download success rate: {download_rate:.1f}%\")\n",
    "        print(f\"   Parse success rate: {parse_rate:.1f}%\")\n",
    "    \n",
    "    if results.get('errors'):\n",
    "        print(\"\\nErrors encountered (showing graceful error handling):\")\n",
    "        for error in results.get('errors', [])[:3]:  # Show first 3 errors\n",
    "            print(f\"   - {error}\")\n",
    "    \n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        print(\"\\nâœ“ Enhanced pipeline test successful!\")\n",
    "        if results.get('errors'):\n",
    "            print(\"âœ“ System continued processing despite PDF failures\")\n",
    "    else:\n",
    "        print(\"\\n! No papers fetched - may be arXiv API unavailability\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
