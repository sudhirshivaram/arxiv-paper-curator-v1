{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Keyword Search First - The Critical Foundation\n",
    "\n",
    "> ** The 90% Problem:** Most RAG systems jump straight to vector search and miss the foundation that powers the best retrieval systems. We're doing it right!\n",
    "\n",
    "## ESSENTIAL SETUP - Do This First!\n",
    "\n",
    "**Before running any cells, ensure your environment is properly configured:**\n",
    "\n",
    "```bash\n",
    "# 1. CRITICAL: Copy the environment configuration\n",
    "cp .env.example .env\n",
    "\n",
    "# 2. Verify these Week 3 settings are in your .env:\n",
    "# OPENSEARCH__HOST=http://opensearch:9200\n",
    "# OPENSEARCH__INDEX_NAME=arxiv-papers\n",
    "# ARXIV__MAX_RESULTS=15\n",
    "```\n",
    "\n",
    "**Important:** Week 3 requires the `.env` file for OpenSearch connectivity and service configuration. The defaults in `.env.example` work perfectly out of the box!\n",
    "\n",
    "**Why Keyword Search First?**\n",
    "- **Exact Match Power:** Find specific technical terms and paper IDs precisely\n",
    "- **Speed & Efficiency:** BM25 is fast and doesn't require expensive embedding models\n",
    "- **Interpretable:** You understand exactly why papers were retrieved\n",
    "- **Production Reality:** Companies like Elasticsearch use keyword search as their foundation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: OpenSearch Integration & BM25 Search\n",
    "\n",
    "**What We're Building This Week:**\n",
    "\n",
    "Week 3 focuses on implementing OpenSearch integration for full-text search capabilities using BM25 scoring. This transforms our system from a simple storage solution into a searchable knowledge base.\n",
    "\n",
    "## Week 3 Focus Areas\n",
    "\n",
    "### Core Objectives\n",
    "- **OpenSearch Integration**: Connect our FastAPI application to OpenSearch cluster\n",
    "- **Index Management**: Create and manage the arxiv-papers index with proper mappings\n",
    "- **BM25 Search**: Implement full-text search with relevance scoring\n",
    "- **Data Pipeline**: Transfer papers from PostgreSQL to OpenSearch\n",
    "- **Search API**: Expose search functionality through REST endpoints\n",
    "\n",
    "### What We'll Test In This Notebook\n",
    "1. **Infrastructure Verification** - Ensure all services from Week 1-2 are running\n",
    "2. **OpenSearch Service Integration** - Test client creation and health checks\n",
    "3. **Index Creation & Management** - Create arxiv-papers index with proper mappings\n",
    "4. **Data Pipeline** - Transfer papers from PostgreSQL to OpenSearch\n",
    "5. **BM25 Search Functionality** - Test search queries with relevance scoring\n",
    "6. **Search API Endpoints** - Verify FastAPI search endpoints work correctly\n",
    "\n",
    "### Success Metrics\n",
    "- OpenSearch cluster healthy and accessible\n",
    "- arxiv-papers index created with proper mappings\n",
    "- Papers successfully indexed from PostgreSQL\n",
    "- BM25 search returns relevant results with scores\n",
    "- Search API endpoints respond correctly\n",
    "- All components ready for production use\n",
    "\n",
    "---\n",
    "\n",
    "## Week 3 Component Status\n",
    "| Component | Purpose | Status |\n",
    "|-----------|---------|--------|\n",
    "| **OpenSearch Client** | Connect to OpenSearch cluster | ✅ Complete |\n",
    "| **Index Management** | Create and manage search indices | ✅ Complete |\n",
    "| **Query Builder** | Build complex search queries | ✅ Complete |\n",
    "| **Data Pipeline** | Transfer papers to OpenSearch | ✅ Complete |\n",
    "| **Search API** | REST endpoints for search | ✅ Complete |\n",
    "| **BM25 Scoring** | Relevance-based search results | ✅ Complete |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Week 3 Docker Services Restart\n",
    "\n",
    "**NEW USERS OR INTEGRATION CONFLICTS**: Week 3 introduces OpenSearch integration that requires fresh container state. Use this clean restart approach:\n",
    "\n",
    "### Fresh Start (Recommended for Week 3)\n",
    "```bash\n",
    "# Complete clean slate - removes all data but ensures correct OpenSearch state\n",
    "docker compose down -v\n",
    "\n",
    "# Build fresh containers with latest code\n",
    "docker compose up --build -d\n",
    "```\n",
    "\n",
    "**When to use this:**\n",
    "- First time running Week 3 \n",
    "- OpenSearch connection issues\n",
    "- Index conflicts or mapping errors\n",
    "- Want to start with clean OpenSearch state\n",
    "\n",
    "**Note**: This destroys existing data but ensures you have the correct Week 3 configuration with proper OpenSearch integration.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Check\n",
    "\n",
    "**Before starting:**\n",
    "1. Week 1 infrastructure completed\n",
    "2. Week 2 arXiv integration working\n",
    "3. UV environment activated\n",
    "4. Docker Desktop running\n",
    "5. Some papers already in PostgreSQL from Week 2\n",
    "\n",
    "**Why fresh containers?** Week 3 includes OpenSearch integration that requires proper cluster initialization and may conflict with existing index states.\n",
    "\n",
    "**Service Access Points:**\n",
    "- **FastAPI**: http://localhost:8000/docs (API documentation)\n",
    "- **PostgreSQL**: via API or `docker exec -it rag-postgres psql -U rag_user -d rag_db`\n",
    "- **OpenSearch**: http://localhost:9200/_cluster/health\n",
    "- **Ollama**: http://localhost:11434 (LLM service)\n",
    "- **Airflow**: http://localhost:8080 (Username: `admin`, Password: `admin`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.3\n",
      "Environment: /home/bhargav/arxiv-paper-curator/.venv/bin/python\n",
      "Project root: /home/bhargav/arxiv-paper-curator\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Path Configuration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests # type: ignore\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Environment: {sys.executable}\")\n",
    "\n",
    "# Find project root and add to Python path\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week3\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = None\n",
    "\n",
    "if project_root and (project_root / \"compose.yml\").exists():\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"Missing compose.yml - check directory\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Infrastructure Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 3 PREREQUISITE CHECK\n",
      "==================================================\n",
      "✓ FastAPI: Healthy\n",
      "✓ PostgreSQL (via API): Healthy\n",
      "✓ OpenSearch: Healthy\n",
      "✓ Airflow: Healthy\n",
      "\n",
      "All services healthy! Ready for Week 3 OpenSearch integration.\n"
     ]
    }
   ],
   "source": [
    "# Service Health Verification\n",
    "print(\"WEEK 3 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\", \n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\"  \n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"✗ {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"✗ {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "print()\n",
    "if all_healthy:\n",
    "    print(\"All services healthy! Ready for Week 3 OpenSearch integration.\")\n",
    "else:\n",
    "    print(\"Some services need attention. Please run: docker compose up --build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenSearch Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENSEARCH CLIENT SETUP\n",
      "========================================\n",
      "Client configured with host: http://localhost:9200\n",
      "Index name: arxiv-papers-chunks\n",
      "✓ OpenSearch health check: PASSED\n"
     ]
    }
   ],
   "source": [
    "# OpenSearch Client Setup\n",
    "from src.services.opensearch.factory import make_opensearch_client\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "print(\"OPENSEARCH CLIENT SETUP\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create OpenSearch client using factory pattern\n",
    "opensearch_client = make_opensearch_client()\n",
    "\n",
    "# Override for notebook execution (localhost instead of container hostname)\n",
    "opensearch_client.host = \"http://localhost:9200\"\n",
    "opensearch_client.client = OpenSearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    "    http_compress=True,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "print(f\"Client configured with host: {opensearch_client.host}\")\n",
    "print(f\"Index name: {opensearch_client.index_name}\")\n",
    "\n",
    "# Test health check\n",
    "is_healthy = opensearch_client.health_check()\n",
    "if is_healthy:\n",
    "    print(\"✓ OpenSearch health check: PASSED\")\n",
    "else:\n",
    "    print(\"✗ OpenSearch health check: FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX CONFIGURATION\n",
      "========================================\n",
      "Index Name: arxiv-papers-chunks\n",
      "\n",
      "Key Features:\n",
      "• Custom text analyzers for better search\n",
      "• BM25 scoring for relevance\n",
      "• Vector embeddings for semantic search (1024 dimensions)\n",
      "• Hybrid search with RRF (Reciprocal Rank Fusion)\n",
      "• HNSW algorithm for fast vector search\n",
      "\n",
      "Mapping Structure:\n",
      "{\n",
      "  \"settings\": {\n",
      "    \"number_of_shards\": 1,\n",
      "    \"number_of_replicas\": 0,\n",
      "    \"index.knn\": true,\n",
      "    \"index.knn.space_type\": \"cosinesimil\",\n",
      "    \"analysis\": {\n",
      "      \"analyzer\": {\n",
      "        \"standard_analyzer\": {\n",
      "          \"type\": \"standard\",\n",
      "          \"stopwords\": \"_english_\"\n",
      "        },\n",
      "        \"text_analyzer\": {\n",
      "          \"type\": \"custom\",\n",
      "          \"tokenizer\": \"standard\",\n",
      "          \"filter\": [\n",
      "            \"lowercase\",\n",
      "            \"stop\",\n",
      "            \"snowball\"\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"mappings\": {\n",
      "    \"dynamic\": \"strict\",\n",
      "    \"properties\": {\n",
      "      \"chunk_id\": {\n",
      "        \"type\": \"keyword\"\n",
      "      },\n",
      "      \"arxiv_id\": {\n",
      "        \"type\": \"keyword\"\n",
      "      },\n",
      "      \"paper_id\": {\n",
      "        \"type\": \"keyword\"\n",
      "      },\n",
      "      \"chunk_index\": {\n",
      "        \"type\": \"integer\"\n",
      "      },\n",
      "      \"chunk_text\": {\n",
      "        \"type\": \"text\",\n",
      "        \"analyzer\": \"text_analyzer\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"chunk_word_count\": {\n",
      "        \"type\": \"integer\"\n",
      "      },\n",
      "      \"start_char\": {\n",
      "        \"type\": \"integer\"\n",
      "      },\n",
      "      \"end_char\": {\n",
      "        \"type\": \"integer\"\n",
      "      },\n",
      "      \"embedding\": {\n",
      "        \"type\": \"knn_vector\",\n",
      "        \"dimension\": 1024,\n",
      "        \"method\": {\n",
      "          \"name\": \"hnsw\",\n",
      "          \"space_type\": \"cosinesimil\",\n",
      "          \"engine\": \"nmslib\",\n",
      "          \"parameters\": {\n",
      "            \"ef_construction\": 512,\n",
      "            \"m\": 16\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"title\": {\n",
      "        \"type\": \"text\",\n",
      "        \"analyzer\": \"text_analyzer\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"authors\": {\n",
      "        \"type\": \"text\",\n",
      "        \"analyzer\": \"standard_analyzer\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"abstract\": {\n",
      "        \"type\": \"text\",\n",
      "        \"analyzer\": \"text_analyzer\"\n",
      "      },\n",
      "      \"categories\": {\n",
      "        \"type\": \"keyword\"\n",
      "      },\n",
      "      \"published_date\": {\n",
      "        \"type\": \"date\"\n",
      "      },\n",
      "      \"section_title\": {\n",
      "        \"type\": \"keyword\"\n",
      "      },\n",
      "      \"embedding_model\": {\n",
      "        \"type\": \"keyword\"\n",
      "      },\n",
      "      \"created_at\": {\n",
      "        \"type\": \"date\"\n",
      "      },\n",
      "      \"updated_at\": {\n",
      "        \"type\": \"date\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Hybrid RRF Pipeline:\n",
      "{\n",
      "  \"id\": \"hybrid-rrf-pipeline\",\n",
      "  \"description\": \"Post processor for hybrid RRF search\",\n",
      "  \"phase_results_processors\": [\n",
      "    {\n",
      "      \"score-ranker-processor\": {\n",
      "        \"combination\": {\n",
      "          \"technique\": \"rrf\",\n",
      "          \"rank_constant\": 60\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Display Index Configuration\n",
    "# Display Index Configuration\n",
    "from src.services.opensearch.index_config_hybrid import (\n",
    "    ARXIV_PAPERS_CHUNKS_INDEX,\n",
    "    ARXIV_PAPERS_CHUNKS_MAPPING,\n",
    "    HYBRID_RRF_PIPELINE\n",
    ")\n",
    "\n",
    "print(\"INDEX CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Index Name: {ARXIV_PAPERS_CHUNKS_INDEX}\")\n",
    "\n",
    "print(f\"\\nKey Features:\")\n",
    "print(\"• Custom text analyzers for better search\")\n",
    "print(\"• BM25 scoring for relevance\")\n",
    "print(\"• Vector embeddings for semantic search (1024 dimensions)\")\n",
    "print(\"• Hybrid search with RRF (Reciprocal Rank Fusion)\")\n",
    "print(\"• HNSW algorithm for fast vector search\")\n",
    "\n",
    "print(\"\\nMapping Structure:\")\n",
    "print(json.dumps(ARXIV_PAPERS_CHUNKS_MAPPING, indent=2))\n",
    "\n",
    "print(\"\\nHybrid RRF Pipeline:\")\n",
    "print(json.dumps(HYBRID_RRF_PIPELINE, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX CREATION\n",
      "========================================\n",
      "✓ Index 'arxiv-papers-chunks' already exists\n",
      "\n",
      "Current Statistics:\n",
      "   Documents: 0\n",
      "   Size: 208 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create Index if it doesn't exist\n",
    "print(\"INDEX CREATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Check if index already exists\n",
    "    index_exists = opensearch_client.client.indices.exists(index=opensearch_client.index_name)\n",
    "    \n",
    "    if index_exists:\n",
    "        print(f\"✓ Index '{opensearch_client.index_name}' already exists\")\n",
    "        \n",
    "        # Get current index statistics\n",
    "        stats = opensearch_client.get_index_stats()\n",
    "        if stats and 'error' not in stats:\n",
    "            print(f\"\\nCurrent Statistics:\")\n",
    "            print(f\"   Documents: {stats.get('document_count', 0)}\")\n",
    "            print(f\"   Size: {stats.get('size_in_bytes', 0):,} bytes\")\n",
    "    else:\n",
    "        print(f\"Creating new index: {opensearch_client.index_name}\")\n",
    "        \n",
    "        # Create the index with our custom mapping\n",
    "        success = opensearch_client.create_index()\n",
    "        \n",
    "        if success:\n",
    "            print(f\"✓ Index created successfully!\")\n",
    "        else:\n",
    "            print(f\"✗ Index creation failed\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error with index management: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Pipeline - Run Airflow DAG\n",
    "\n",
    "The **arxiv_paper_ingestion** DAG automatically:\n",
    "1. Fetches papers from arXiv API\n",
    "2. Stores papers in PostgreSQL\n",
    "3. **Indexes papers into OpenSearch**\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Before proceeding, run the Airflow DAG:**\n",
    "\n",
    "1. Open Airflow UI: http://localhost:8080\n",
    "2. Login: username `admin`, password `admin`\n",
    "3. Find **`arxiv_paper_ingestion`** DAG\n",
    "4. Click the DAG name to open it\n",
    "5. Click **\"Trigger DAG\"** button (▶️ play icon)\n",
    "6. Wait ~10 minutes for completion\n",
    "7. Check that all tasks turn green\n",
    "\n",
    "Then run the cell below to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFYING DATA PIPELINE\n",
      "========================================\n",
      "⚠️  No documents in OpenSearch yet\n",
      "\n",
      "Please run the Airflow DAG first (see instructions above)\n"
     ]
    }
   ],
   "source": [
    "# Verify Data Pipeline Results\n",
    "print(\"VERIFYING DATA PIPELINE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "stats = opensearch_client.get_index_stats()\n",
    "\n",
    "if stats and 'error' not in stats:\n",
    "    doc_count = stats.get('document_count', 0)\n",
    "    \n",
    "    if doc_count > 0:\n",
    "        print(f\"✓ Success! Found {doc_count} documents in OpenSearch\")\n",
    "        \n",
    "        # Show sample papers\n",
    "        sample = opensearch_client.search_papers(\"*\", size=3)\n",
    "        if sample.get('hits'):\n",
    "            print(f\"\\nSample papers:\")\n",
    "            for i, paper in enumerate(sample['hits'], 1):\n",
    "                title = paper.get('title', 'Unknown')[:60]\n",
    "                print(f\"  {i}. {title}...\")\n",
    "    else:\n",
    "        print(\"⚠️  No documents in OpenSearch yet\")\n",
    "        print(\"\\nPlease run the Airflow DAG first (see instructions above)\")\n",
    "else:\n",
    "    print(\"✗ Could not retrieve index stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple BM25 Search\n",
    "\n",
    "Let's start with a simple search to demonstrate BM25 scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE BM25 SEARCH\n",
      "========================================\n",
      "Searching for: 'learning'\n",
      "\n",
      "No results found. Try searching for:\n",
      "  • 'neural', 'model', 'algorithm'\n",
      "  • Use '*' to see all papers\n"
     ]
    }
   ],
   "source": [
    "# Simple BM25 Search\n",
    "print(\"SIMPLE BM25 SEARCH\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Change this to any word from your papers\n",
    "search_term = \"learning\"  # Try different terms!\n",
    "\n",
    "print(f\"Searching for: '{search_term}'\\n\")\n",
    "\n",
    "results = opensearch_client.search_papers(\n",
    "    query=search_term,\n",
    "    size=5\n",
    ")\n",
    "\n",
    "if results.get('hits'):\n",
    "    print(f\"Found {results.get('total', 0)} total matches\\n\")\n",
    "    \n",
    "    for i, paper in enumerate(results['hits'], 1):\n",
    "        print(f\"{i}. {paper.get('title', 'Unknown')[:70]}...\")\n",
    "        print(f\"   Score: {paper.get('score', 0):.2f}\")\n",
    "        print(f\"   arXiv ID: {paper.get('arxiv_id', 'N/A')}\\n\")\n",
    "else:\n",
    "    print(\"No results found. Try searching for:\")\n",
    "    print(\"  • 'neural', 'model', 'algorithm'\")\n",
    "    print(\"  • Use '*' to see all papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced OpenSearch Queries\n",
    "\n",
    "Now let's explore different query types using the OpenSearch Python client directly. This shows the power of BM25 without needing vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Match Query\n",
    "\n",
    "The `match` query is the standard query for full-text search on a single field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH QUERY - Single Field Search\n",
      "========================================\n",
      "Found 0 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Match Query - Search in title field\n",
    "print(\"MATCH QUERY - Single Field Search\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"title\": \"machine learning\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Title: {hit['_source']['title'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Multi-Match Query\n",
    "\n",
    "Search across multiple fields simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-MATCH QUERY - Search Multiple Fields\n",
      "========================================\n",
      "Found 0 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-Match Query - Search across multiple fields\n",
    "print(\"MULTI-MATCH QUERY - Search Multiple Fields\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": \"AI Agents\",\n",
    "            \"fields\": [\"title^2\", \"abstract\", \"authors\"],  # ^2 boosts title field\n",
    "            \"type\": \"best_fields\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Title: {hit['_source']['title'][:70]}...\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\")\n",
    "    print(f\"Authors: {', '.join(hit['_source']['authors'][:2])}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Boosting Query\n",
    "\n",
    "Boost certain results while demoting others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOSTING QUERY - Promote/Demote Results\n",
      "========================================\n",
      "Query: Boost 'deep learning', demote 'survey' papers\n",
      "\n",
      "Found 0 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boosting Query - Promote and demote results\n",
    "print(\"BOOSTING QUERY - Promote/Demote Results\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"boosting\": {\n",
    "            \"positive\": {\n",
    "                \"match\": {\n",
    "                    \"abstract\": \"deep learning\"\n",
    "                }\n",
    "            },\n",
    "            \"negative\": {\n",
    "                \"match\": {\n",
    "                    \"abstract\": \"multimodal\"\n",
    "                }\n",
    "            },\n",
    "            \"negative_boost\": 0.1  # Reduce score of negative matches\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Query: Boost 'deep learning', demote 'survey' papers\\n\")\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    abstract_snippet = hit['_source']['abstract'][:100]\n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\")\n",
    "    print(f\"Abstract: {abstract_snippet}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Filter Query\n",
    "\n",
    "Filter results by specific criteria (doesn't affect scoring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTER QUERY - Category Filtering\n",
      "========================================\n",
      "Found 0 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter Query - Filter by categories\n",
    "print(\"FILTER QUERY - Category Filtering\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"abstract\": \"neural\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"filter\": [\n",
    "                {\n",
    "                    \"terms\": {\n",
    "                        \"categories\": [\"cs.AI\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    categories = ', '.join(hit['_source']['categories'])\n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"Categories: {categories}\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Sorting Query\n",
    "\n",
    "Sort results by different criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SORTING QUERY - Latest Papers First\n",
      "========================================\n",
      "Query: All papers sorted by publication date (newest first)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting Query - Sort by publication date\n",
    "print(\"SORTING QUERY - Latest Papers First\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}  # Get all papers\n",
    "    },\n",
    "    \"sort\": [\n",
    "        {\n",
    "            \"published_date\": {\n",
    "                \"order\": \"desc\"  # Latest first\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"size\": 5\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Query: All papers sorted by publication date (newest first)\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    pub_date = hit['_source']['published_date'][:10]\n",
    "    print(f\"Date: {pub_date} | {title}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Combined Query\n",
    "\n",
    "Combine multiple query types for complex searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED QUERY - Complex Search\n",
      "========================================\n",
      "Complex Query:\n",
      "  • Must contain 'transformer' (title boosted 3x)\n",
      "  • Filter: published after 2024-01-01\n",
      "  • Prefer: cs.AI category\n",
      "  • Sort: by relevance, then date\n",
      "\n",
      "Found 0 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Query - Complex search with multiple criteria\n",
    "print(\"COMBINED QUERY - Complex Search\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": \"transformer\",\n",
    "                        \"fields\": [\"title^3\", \"abstract\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"filter\": [\n",
    "                {\n",
    "                    \"range\": {\n",
    "                        \"published_date\": {\n",
    "                            \"gte\": \"2024-01-01\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"categories\": \"cs.AI\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"sort\": [\n",
    "        \"_score\",\n",
    "        {\"published_date\": {\"order\": \"desc\"}}\n",
    "    ],\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Complex Query:\")\n",
    "print(f\"  • Must contain 'transformer' (title boosted 3x)\")\n",
    "print(f\"  • Filter: published after 2024-01-01\")\n",
    "print(f\"  • Prefer: cs.AI category\")\n",
    "print(f\"  • Sort: by relevance, then date\\n\")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    pub_date = hit['_source']['published_date'][:10]\n",
    "    score = hit['_score']\n",
    "    categories = ', '.join(hit['_source']['categories'][:2])\n",
    "    \n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"  Date: {pub_date} | Score: {score:.2f}\")\n",
    "    print(f\"  Categories: {categories}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "**BM25 Search is Powerful!** Without any vector embeddings, we can:\n",
    "\n",
    "1. **Simple Search**: Basic keyword search with relevance scoring\n",
    "2. **Match Queries**: Search specific fields\n",
    "3. **Multi-Match**: Search across multiple fields with boosting\n",
    "4. **Boosting**: Promote or demote certain results\n",
    "5. **Filtering**: Apply filters without affecting scores\n",
    "6. **Sorting**: Order results by date, score, or other fields\n",
    "7. **Complex Queries**: Combine all techniques for sophisticated searches\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **BM25 works great** for many search use cases\n",
    "- **No vectors needed** for effective full-text search\n",
    "- **Simple and fast** compared to embedding-based approaches\n",
    "- **Filters and sorting** make searches precise and relevant\n",
    "- **Field boosting** helps prioritize important content\n",
    "\n",
    "### When to Use BM25 vs Vectors\n",
    "\n",
    "**Use BM25 when:**\n",
    "- Searching for specific keywords or phrases\n",
    "- Need fast, simple implementation\n",
    "- Have good text fields with clear terminology\n",
    "- Want explainable search results\n",
    "\n",
    "**Consider vectors when:**\n",
    "- Need semantic similarity (concepts, not keywords)\n",
    "- Dealing with synonyms and paraphrasing\n",
    "- Cross-language search requirements\n",
    "- Very short queries or documents\n",
    "\n",
    "Remember: **You can also combine both** (hybrid search) for best results!\n",
    "We will see this in the next week :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexed chunks: 0\n",
      "✓ Papers in database: 0\n"
     ]
    }
   ],
   "source": [
    "# In your Week 3 notebook or new cell:\n",
    "import requests\n",
    "\n",
    "# Check OpenSearch\n",
    "response = requests.get(\"http://localhost:9200/arxiv-papers-chunks/_count\")\n",
    "chunk_count = response.json()['count']\n",
    "print(f\"✓ Indexed chunks: {chunk_count}\")\n",
    "\n",
    "# Check PostgreSQL\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    port=5432,\n",
    "    database='rag_db',\n",
    "    user='rag_user',\n",
    "    password='rag_password'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM papers\")\n",
    "paper_count = cursor.fetchone()[0]\n",
    "print(f\"✓ Papers in database: {paper_count}\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    port=5432,\n",
    "    database='rag_db',\n",
    "    user='rag_user',\n",
    "    password='rag_password'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check if papers have text content\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT id, arxiv_id, title, \n",
    "           LENGTH(raw_text) as text_length,\n",
    "           LENGTH(abstract) as abstract_length\n",
    "    FROM papers \n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"Paper {row[1]}: text={row[3]} chars, abstract={row[4]} chars\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent Task Results:\n",
      "================================================================================\n",
      "\n",
      "Task: setup_environment\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:49:30.868918+00:00\n",
      "Value: \\x7b22737461747573223a202273756363657373222c20226d657373616765223a2022456e7669726f6e6d656e7420736574757020636f6d706c65746564227d...\n",
      "\n",
      "Task: cleanup_temp_files\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:44:19.333480+00:00\n",
      "Value: \\x22436c65616e757020636f6d706c6574656422...\n",
      "\n",
      "Task: generate_daily_report\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:44:17.012207+00:00\n",
      "Value: \\x7b22657865637574696f6e5f64617465223a2022323032352d31312d32385431373a34333a32352b30303a3030222c202266657463685f73746174697374696373223a207b227061706572735f66657463686564223a20332c20227061706572735f73746f726564223a20332c20227461726765745f64617465223a206e756c6c7d2c2022696e646578696e675f73746174697374696373223a207b227061706572735f70726f636573736564223a20302c20226368756e6b735f63726561746564223a20302c20226368756e6b735f696e6465786564223a20302c2022656d62656464696e67735f67656e657261746564223a20307d2c20...\n",
      "\n",
      "Task: generate_daily_report\n",
      "Key: daily_report\n",
      "Time: 2025-11-28 17:44:16.953734+00:00\n",
      "Value: \\x7b22657865637574696f6e5f64617465223a2022323032352d31312d32385431373a34333a32352b30303a3030222c202266657463685f73746174697374696373223a207b227061706572735f66657463686564223a20332c20227061706572735f73746f726564223a20332c20227461726765745f64617465223a206e756c6c7d2c2022696e646578696e675f73746174697374696373223a207b227061706572735f70726f636573736564223a20302c20226368756e6b735f63726561746564223a20302c20226368756e6b735f696e6465786564223a20302c2022656d62656464696e67735f67656e657261746564223a20307d2c20...\n",
      "\n",
      "Task: index_papers_hybrid\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:44:00.351962+00:00\n",
      "Value: \\x7b227061706572735f70726f636573736564223a20302c2022746f74616c5f6368756e6b735f63726561746564223a20302c2022746f74616c5f6368756e6b735f696e6465786564223a20302c2022746f74616c5f656d62656464696e67735f67656e657261746564223a20302c2022746f74616c5f6572726f7273223a20307d...\n",
      "\n",
      "Task: fetch_daily_papers\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:43:57.613677+00:00\n",
      "Value: \\x7b227061706572735f66657463686564223a20332c2022706466735f646f776e6c6f61646564223a20302c2022706466735f706172736564223a20302c20227061706572735f73746f726564223a20332c20227061706572735f696e6465786564223a20302c20226572726f7273223a205b22506970656c696e65206572726f7220666f7220323531312e323136393276313a20506970656c696e65206572726f7220666f7220323531312e323136393276313a20446f636c696e672070617273696e672072657475726e6564206e6f20726573756c7420666f7220323531312e323136393276312e706466222c2022506970656c696e6520...\n",
      "\n",
      "Task: fetch_daily_papers\n",
      "Key: fetch_results\n",
      "Time: 2025-11-28 17:43:57.550364+00:00\n",
      "Value: \\x7b227061706572735f66657463686564223a20332c2022706466735f646f776e6c6f61646564223a20302c2022706466735f706172736564223a20302c20227061706572735f73746f726564223a20332c20227061706572735f696e6465786564223a20302c20226572726f7273223a205b22506970656c696e65206572726f7220666f7220323531312e323136393276313a20506970656c696e65206572726f7220666f7220323531312e323136393276313a20446f636c696e672070617273696e672072657475726e6564206e6f20726573756c7420666f7220323531312e323136393276312e706466222c2022506970656c696e6520...\n",
      "\n",
      "Task: setup_environment\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:43:40.746852+00:00\n",
      "Value: \\x7b22737461747573223a202273756363657373222c20226d657373616765223a2022456e7669726f6e6d656e7420736574757020636f6d706c65746564227d...\n",
      "\n",
      "Task: cleanup_temp_files\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:39:20.823671+00:00\n",
      "Value: \\x22436c65616e757020636f6d706c6574656422...\n",
      "\n",
      "Task: generate_daily_report\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:39:18.128422+00:00\n",
      "Value: \\x7b22657865637574696f6e5f64617465223a2022323032352d31312d32385431373a33343a30362b30303a3030222c202266657463685f73746174697374696373223a207b227061706572735f66657463686564223a20332c20227061706572735f73746f726564223a20332c20227461726765745f64617465223a206e756c6c7d2c2022696e646578696e675f73746174697374696373223a207b227061706572735f70726f636573736564223a20302c20226368756e6b735f63726561746564223a20302c20226368756e6b735f696e6465786564223a20302c2022656d62656464696e67735f67656e657261746564223a20307d2c20...\n",
      "\n",
      "Task: generate_daily_report\n",
      "Key: daily_report\n",
      "Time: 2025-11-28 17:39:18.064428+00:00\n",
      "Value: \\x7b22657865637574696f6e5f64617465223a2022323032352d31312d32385431373a33343a30362b30303a3030222c202266657463685f73746174697374696373223a207b227061706572735f66657463686564223a20332c20227061706572735f73746f726564223a20332c20227461726765745f64617465223a206e756c6c7d2c2022696e646578696e675f73746174697374696373223a207b227061706572735f70726f636573736564223a20302c20226368756e6b735f63726561746564223a20302c20226368756e6b735f696e6465786564223a20302c2022656d62656464696e67735f67656e657261746564223a20307d2c20...\n",
      "\n",
      "Task: index_papers_hybrid\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:38:54.729733+00:00\n",
      "Value: \\x7b227061706572735f70726f636573736564223a20302c2022746f74616c5f6368756e6b735f63726561746564223a20302c2022746f74616c5f6368756e6b735f696e6465786564223a20302c2022746f74616c5f656d62656464696e67735f67656e657261746564223a20302c2022746f74616c5f6572726f7273223a20307d...\n",
      "\n",
      "Task: fetch_daily_papers\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:38:50.801668+00:00\n",
      "Value: \\x7b227061706572735f66657463686564223a20332c2022706466735f646f776e6c6f61646564223a20302c2022706466735f706172736564223a20302c20227061706572735f73746f726564223a20332c20227061706572735f696e6465786564223a20302c20226572726f7273223a205b22506970656c696e65206572726f7220666f7220323531312e323136393276313a20506970656c696e65206572726f7220666f7220323531312e323136393276313a20446f636c696e672070617273696e672072657475726e6564206e6f20726573756c7420666f7220323531312e323136393276312e706466222c2022506970656c696e6520...\n",
      "\n",
      "Task: fetch_daily_papers\n",
      "Key: fetch_results\n",
      "Time: 2025-11-28 17:38:50.738084+00:00\n",
      "Value: \\x7b227061706572735f66657463686564223a20332c2022706466735f646f776e6c6f61646564223a20302c2022706466735f706172736564223a20302c20227061706572735f73746f726564223a20332c20227061706572735f696e6465786564223a20302c20226572726f7273223a205b22506970656c696e65206572726f7220666f7220323531312e323136393276313a20506970656c696e65206572726f7220666f7220323531312e323136393276313a20446f636c696e672070617273696e672072657475726e6564206e6f20726573756c7420666f7220323531312e323136393276312e706466222c2022506970656c696e6520...\n",
      "\n",
      "Task: setup_environment\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:38:21.589473+00:00\n",
      "Value: \\x7b22737461747573223a202273756363657373222c20226d657373616765223a2022456e7669726f6e6d656e7420736574757020636f6d706c65746564227d...\n",
      "\n",
      "Task: setup_environment\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:27:32.158278+00:00\n",
      "Value: \\x7b22737461747573223a202273756363657373222c20226d657373616765223a2022456e7669726f6e6d656e7420736574757020636f6d706c65746564227d...\n",
      "\n",
      "Task: setup_environment\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:21:26.495100+00:00\n",
      "Value: \\x7b22737461747573223a202273756363657373222c20226d657373616765223a2022456e7669726f6e6d656e7420736574757020636f6d706c65746564227d...\n",
      "\n",
      "Task: cleanup_temp_files\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:15:41.313254+00:00\n",
      "Value: \\x22436c65616e757020636f6d706c6574656422...\n",
      "\n",
      "Task: generate_daily_report\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:15:39.373823+00:00\n",
      "Value: \\x7b22657865637574696f6e5f64617465223a2022323032352d31312d32385431373a31343a35352e3136323433382b30303a3030222c202266657463685f73746174697374696373223a207b227061706572735f66657463686564223a20302c20227061706572735f73746f726564223a20302c20227461726765745f64617465223a20223230323531313238227d2c2022696e646578696e675f73746174697374696373223a207b227061706572735f70726f636573736564223a20302c20226368756e6b735f63726561746564223a20302c20226368756e6b735f696e6465786564223a20302c2022656d62656464696e67735f67656e...\n",
      "\n",
      "Task: generate_daily_report\n",
      "Key: daily_report\n",
      "Time: 2025-11-28 17:15:39.322274+00:00\n",
      "Value: \\x7b22657865637574696f6e5f64617465223a2022323032352d31312d32385431373a31343a35352e3136323433382b30303a3030222c202266657463685f73746174697374696373223a207b227061706572735f66657463686564223a20302c20227061706572735f73746f726564223a20302c20227461726765745f64617465223a20223230323531313238227d2c2022696e646578696e675f73746174697374696373223a207b227061706572735f70726f636573736564223a20302c20226368756e6b735f63726561746564223a20302c20226368756e6b735f696e6465786564223a20302c2022656d62656464696e67735f67656e...\n",
      "\n",
      "Task: index_papers_hybrid\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:15:26.067046+00:00\n",
      "Value: \\x7b227061706572735f70726f636573736564223a20302c2022746f74616c5f6368756e6b735f63726561746564223a20302c2022746f74616c5f6368756e6b735f696e6465786564223a20302c2022746f74616c5f656d62656464696e67735f67656e657261746564223a20302c2022746f74616c5f6572726f7273223a20307d...\n",
      "\n",
      "Task: fetch_daily_papers\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:15:23.940850+00:00\n",
      "Value: \\x7b227061706572735f66657463686564223a20302c2022706466735f646f776e6c6f61646564223a20302c2022706466735f706172736564223a20302c20227061706572735f73746f726564223a20302c20227061706572735f696e6465786564223a20302c20226572726f7273223a205b5d2c202270726f63657373696e675f74696d65223a20302c202264617465223a20223230323531313238227d...\n",
      "\n",
      "Task: fetch_daily_papers\n",
      "Key: fetch_results\n",
      "Time: 2025-11-28 17:15:23.896371+00:00\n",
      "Value: \\x7b227061706572735f66657463686564223a20302c2022706466735f646f776e6c6f61646564223a20302c2022706466735f706172736564223a20302c20227061706572735f73746f726564223a20302c20227061706572735f696e6465786564223a20302c20226572726f7273223a205b5d2c202270726f63657373696e675f74696d65223a20302c202264617465223a20223230323531313238227d...\n",
      "\n",
      "Task: setup_environment\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:15:09.669790+00:00\n",
      "Value: \\x7b22737461747573223a202273756363657373222c20226d657373616765223a2022456e7669726f6e6d656e7420736574757020636f6d706c65746564227d...\n",
      "\n",
      "Task: cleanup_temp_files\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:12:24.753057+00:00\n",
      "Value: \\x22436c65616e757020636f6d706c6574656422...\n",
      "\n",
      "Task: generate_daily_report\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:12:22.583901+00:00\n",
      "Value: \\x7b22657865637574696f6e5f64617465223a2022323032352d31312d32385431373a31313a33352e3834373636362b30303a3030222c202266657463685f73746174697374696373223a207b227061706572735f66657463686564223a20302c20227061706572735f73746f726564223a20302c20227461726765745f64617465223a20223230323531313237227d2c2022696e646578696e675f73746174697374696373223a207b227061706572735f70726f636573736564223a20302c20226368756e6b735f63726561746564223a20302c20226368756e6b735f696e6465786564223a20302c2022656d62656464696e67735f67656e...\n",
      "\n",
      "Task: generate_daily_report\n",
      "Key: daily_report\n",
      "Time: 2025-11-28 17:12:22.528767+00:00\n",
      "Value: \\x7b22657865637574696f6e5f64617465223a2022323032352d31312d32385431373a31313a33352e3834373636362b30303a3030222c202266657463685f73746174697374696373223a207b227061706572735f66657463686564223a20302c20227061706572735f73746f726564223a20302c20227461726765745f64617465223a20223230323531313237227d2c2022696e646578696e675f73746174697374696373223a207b227061706572735f70726f636573736564223a20302c20226368756e6b735f63726561746564223a20302c20226368756e6b735f696e6465786564223a20302c2022656d62656464696e67735f67656e...\n",
      "\n",
      "Task: index_papers_hybrid\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:12:08.873221+00:00\n",
      "Value: \\x7b227061706572735f70726f636573736564223a20302c2022746f74616c5f6368756e6b735f63726561746564223a20302c2022746f74616c5f6368756e6b735f696e6465786564223a20302c2022746f74616c5f656d62656464696e67735f67656e657261746564223a20302c2022746f74616c5f6572726f7273223a20307d...\n",
      "\n",
      "Task: fetch_daily_papers\n",
      "Key: return_value\n",
      "Time: 2025-11-28 17:12:06.140111+00:00\n",
      "Value: \\x7b227061706572735f66657463686564223a20302c2022706466735f646f776e6c6f61646564223a20302c2022706466735f706172736564223a20302c20227061706572735f73746f726564223a20302c20227061706572735f696e6465786564223a20302c20226572726f7273223a205b5d2c202270726f63657373696e675f74696d65223a20302c202264617465223a20223230323531313237227d...\n",
      "\n",
      "Task: fetch_daily_papers\n",
      "Key: fetch_results\n",
      "Time: 2025-11-28 17:12:06.087976+00:00\n",
      "Value: \\x7b227061706572735f66657463686564223a20302c2022706466735f646f776e6c6f61646564223a20302c2022706466735f706172736564223a20302c20227061706572735f73746f726564223a20302c20227061706572735f696e6465786564223a20302c20226572726f7273223a205b5d2c202270726f63657373696e675f74696d65223a20302c202264617465223a20223230323531313237227d...\n"
     ]
    }
   ],
   "source": [
    "# In your notebook - this will show task results\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost', port=5432,\n",
    "    database='rag_db', user='rag_user', password='rag_password'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get recent task results from XCom\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT task_id, key, value::text, timestamp\n",
    "    FROM xcom \n",
    "    WHERE dag_id = 'arxiv_paper_ingestion'\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 30\n",
    "\"\"\")\n",
    "\n",
    "print(\"Recent Task Results:\")\n",
    "print(\"=\" * 80)\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"\\nTask: {row[0]}\")\n",
    "    print(f\"Key: {row[1]}\")\n",
    "    print(f\"Time: {row[3]}\")\n",
    "    print(f\"Value: {row[2][:500]}...\")  # First 500 chars\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 0 abstracts!\n"
     ]
    }
   ],
   "source": [
    "# Run this in your Week 3 notebook\n",
    "import psycopg2\n",
    "from opensearchpy import OpenSearch\n",
    "from datetime import datetime\n",
    "\n",
    "conn = psycopg2.connect(host='localhost', port=5432, database='rag_db', user='rag_user', password='rag_password')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT id, arxiv_id, title, authors, abstract, categories, published_date FROM papers WHERE abstract IS NOT NULL\")\n",
    "papers = cursor.fetchall()\n",
    "conn.close()\n",
    "\n",
    "client = OpenSearch(hosts=[{'host': 'localhost', 'port': 9200}], use_ssl=False)\n",
    "\n",
    "for paper in papers:\n",
    "    doc = {\n",
    "        'chunk_id': f\"{paper[1]}_abstract\", 'arxiv_id': paper[1], 'paper_id': str(paper[0]),\n",
    "        'chunk_index': 0, 'chunk_text': paper[4], 'chunk_word_count': len(paper[4].split()),\n",
    "        'title': paper[2], 'authors': paper[3], 'abstract': paper[4],\n",
    "        'categories': paper[5], 'published_date': paper[6],\n",
    "        'section_title': 'Abstract', 'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "    client.index(index='arxiv-papers-chunks', id=doc['chunk_id'], body=doc, refresh=True)\n",
    "\n",
    "print(f\"✅ Indexed {client.count(index='arxiv-papers-chunks')['count']} abstracts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 results\n",
      "\n",
      "Top results:\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "\n",
    "client = OpenSearch(hosts=[{'host': 'localhost', 'port': 9200}], use_ssl=False)\n",
    "\n",
    "# Test BM25 search\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"chunk_text\": \"transformer neural network\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 5\n",
    "}\n",
    "\n",
    "results = client.search(index='arxiv-papers-chunks', body=query)\n",
    "\n",
    "print(f\"Found {results['hits']['total']['value']} results\")\n",
    "print(\"\\nTop results:\")\n",
    "for hit in results['hits']['hits']:\n",
    "    print(f\"\\nScore: {hit['_score']:.2f}\")\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(f\"Text: {hit['_source']['chunk_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers with full text:\n",
      "\n",
      "✓ 0/0 papers have full text\n",
      "✓ OpenSearch has 0 chunks indexed\n"
     ]
    }
   ],
   "source": [
    "# In your notebook - check if papers have full text now\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    port=5432,\n",
    "    database='rag_db',\n",
    "    user='rag_user',\n",
    "    password='rag_password'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check if papers have text content\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT arxiv_id, \n",
    "           LENGTH(raw_text) as text_length,\n",
    "           LENGTH(abstract) as abstract_length\n",
    "    FROM papers \n",
    "    WHERE raw_text IS NOT NULL\n",
    "    ORDER BY id DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Papers with full text:\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[0]}: text={row[1]} chars, abstract={row[2]} chars\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM papers WHERE raw_text IS NOT NULL\")\n",
    "count_with_text = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM papers\")\n",
    "total_papers = cursor.fetchone()[0]\n",
    "\n",
    "print(f\"\\n✓ {count_with_text}/{total_papers} papers have full text\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Check OpenSearch chunks\n",
    "import requests\n",
    "response = requests.get(\"http://localhost:9200/arxiv-papers-chunks/_count\")\n",
    "print(f\"✓ OpenSearch has {response.json()['count']} chunks indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers with full text:\n",
      "\n",
      "✓ 0 papers have full text!\n",
      "✓ OpenSearch will have chunks after indexing task completes\n"
     ]
    }
   ],
   "source": [
    "# Run this in your notebook\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost', port=5432,\n",
    "    database='rag_db', user='rag_user', password='rag_password'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT arxiv_id, LENGTH(raw_text) as text_length\n",
    "    FROM papers \n",
    "    WHERE raw_text IS NOT NULL\n",
    "    ORDER BY id DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Papers with full text:\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[0]}: {row[1]:,} characters\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM papers WHERE raw_text IS NOT NULL\")\n",
    "count = cursor.fetchone()[0]\n",
    "print(f\"\\n✓ {count} papers have full text!\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Check OpenSearch after indexing task completes\n",
    "import requests\n",
    "response = requests.get(\"http://localhost:9200/arxiv-papers-chunks/_count\")\n",
    "print(f\"✓ OpenSearch will have chunks after indexing task completes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenSearch has 0 chunks indexed\n",
      "\n",
      "Found 0 results\n"
     ]
    }
   ],
   "source": [
    "# After DAG completes, check chunks\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"http://localhost:9200/arxiv-papers-chunks/_count\")\n",
    "chunk_count = response.json()['count']\n",
    "print(f\"✓ OpenSearch has {chunk_count} chunks indexed\")\n",
    "\n",
    "# Search for something in the new papers\n",
    "query = {\n",
    "    \"query\": {\"match\": {\"chunk_text\": \"neural network\"}},\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "client = OpenSearch(hosts=[{'host': 'localhost', 'port': 9200}], use_ssl=False)\n",
    "results = client.search(index='arxiv-papers-chunks', body=query)\n",
    "\n",
    "print(f\"\\nFound {results['hits']['total']['value']} results\")\n",
    "for hit in results['hits']['hits']:\n",
    "    print(f\"\\nScore: {hit['_score']:.2f}\")\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(f\"Text: {hit['_source']['chunk_text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers with text from last 24h: 0\n",
      "\n",
      "Recent papers with text:\n"
     ]
    }
   ],
   "source": [
    "# Check if the 8 parsed papers are being seen by the indexing task\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost', port=5432,\n",
    "    database='rag_db', user='rag_user', password='rag_password'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check what the indexing task queries\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT COUNT(*) \n",
    "    FROM papers \n",
    "    WHERE raw_text IS NOT NULL \n",
    "    AND created_at >= NOW() - INTERVAL '1 day'\n",
    "\"\"\")\n",
    "\n",
    "recent_with_text = cursor.fetchone()[0]\n",
    "print(f\"Papers with text from last 24h: {recent_with_text}\")\n",
    "\n",
    "# Check the 8 papers' created_at dates\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT arxiv_id, created_at, LENGTH(raw_text) \n",
    "    FROM papers \n",
    "    WHERE raw_text IS NOT NULL \n",
    "    ORDER BY created_at DESC \n",
    "    LIMIT 8\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRecent papers with text:\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[0]}: created {row[1]}, {row[2]:,} chars\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the latest DAG run results\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost', port=5432,\n",
    "    database='rag_db', user='rag_user', password='rag_password'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get latest indexing stats from XCom\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT value::text\n",
    "    FROM xcom \n",
    "    WHERE dag_id = 'arxiv_paper_ingestion'\n",
    "      AND task_id = 'index_papers_hybrid'\n",
    "      AND key = 'hybrid_index_stats'\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "result = cursor.fetchone()\n",
    "if result:\n",
    "    import binascii, json\n",
    "    hex_data = result[0][2:]\n",
    "    decoded = binascii.unhexlify(hex_data).decode('utf-8')\n",
    "    data = json.loads(decoded)\n",
    "    print(\"Latest indexing stats:\")\n",
    "    print(json.dumps(data, indent=2))\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks from newly parsed papers (2511.20xxx): 0\n"
     ]
    }
   ],
   "source": [
    "# Check for chunks from the newly parsed papers\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "client = OpenSearch(hosts=[{'host': 'localhost', 'port': 9200}], use_ssl=False)\n",
    "\n",
    "# Search for papers from today's batch (2511.20xxx)\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"prefix\": {\n",
    "            \"arxiv_id\": \"2511.20\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 10\n",
    "}\n",
    "\n",
    "results = client.search(index='arxiv-papers-chunks', body=query)\n",
    "print(f\"Chunks from newly parsed papers (2511.20xxx): {results['hits']['total']['value']}\")\n",
    "\n",
    "for hit in results['hits']['hits']:\n",
    "    print(f\"  {hit['_source']['arxiv_id']}: {hit['_source'].get('chunk_index', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2, binascii, json\n",
    "\n",
    "conn = psycopg2.connect(host='localhost', port=5432, database='rag_db', user='rag_user', password='rag_password')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT value::text FROM xcom \n",
    "    WHERE dag_id = 'arxiv_paper_ingestion'\n",
    "      AND task_id = 'index_papers_hybrid'\n",
    "      AND key = 'hybrid_index_stats'\n",
    "    ORDER BY timestamp DESC LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "result = cursor.fetchone()\n",
    "if result:\n",
    "    hex_data = result[0][2:]\n",
    "    decoded = binascii.unhexlify(hex_data).decode('utf-8')\n",
    "    data = json.loads(decoded)\n",
    "    print(\"\\nIndexing stats:\")\n",
    "    print(json.dumps(data, indent=2))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-paper-curator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
